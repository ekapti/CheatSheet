{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python Cheatsheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekapti/CheatSheet/blob/master/Python_Cheatsheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VJuqeiTVqv_8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "koeeP3hbqv2O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Jupyter Notebook Presentations**\n",
        "\n",
        "View -> Cell Toolbar -> Slideshow\n",
        "\n",
        "On terminal go to the directory where the notebook is present and enter following code\n",
        "jupyter nbconvert *.ipynb --to slides --post serve  \n",
        "\n",
        "''*'' is your notebook name\n",
        "\n",
        "You can use the generated html file or go to port 80000"
      ]
    },
    {
      "metadata": {
        "id": "3m1aPFoA5VcO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Python CheatSheet**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4h8XY3fZ5YX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make the outcome as the first column to make it appear at the top of corr plot\n",
        "cols = list(df)\n",
        "cols.insert(0, cols.pop(cols.index('outcome')))\n",
        "df = df.ix[:, cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmBz83rsuKwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# filter columns not in list\n",
        "df[df.columns[~df.columns.isin(names)]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dH9F08lJrS6h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Categorical Feature Finder**"
      ]
    },
    {
      "metadata": {
        "id": "qe5qmpAt_BLx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This code finds categorical features of a dataset so that we can dummy encode them. \n",
        "\n",
        "def categorical_selector(df): \n",
        "    categorical_vars = []\n",
        "    for i in range(0, df.shape[1]):\n",
        "        var_name = list(df)[i]\n",
        "        if ((df[var_name].nunique() <= 10) & (df[var_name].nunique() > 2)) :\n",
        "            categorical_vars.append(var_name)\n",
        "    return categorical_vars\n",
        "\n",
        "                \n",
        "categorical_vars = categorical_selector(df) \n",
        "\n",
        "# This is the binary version of it\n",
        "\n",
        "def binary_var_finder(dataframe):\n",
        "    binary_vars = []\n",
        "    for i in range(0, dataframe.shape[1]):\n",
        "        var_name = list(dataframe)[i]\n",
        "        if (df[var_name].nunique() == 2):\n",
        "            binary_vars.append(var_name)\n",
        "    return binary_vars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mMDB7y3YrZ0o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Random Forest Hyperparameter Tuning**"
      ]
    },
    {
      "metadata": {
        "id": "ENsOkkuMJ-Fm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Random Forest Hyperparameter Tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "pprint(random_grid)\n",
        "{'bootstrap': [True, False],\n",
        " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        " 'max_features': ['auto', 'sqrt'],\n",
        " 'min_samples_leaf': [1, 2, 4],\n",
        " 'min_samples_split': [2, 5, 10],\n",
        " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_rgdc1Vesb_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**CAP CURVE BUILDER**"
      ]
    },
    {
      "metadata": {
        "id": "Z2yBb_r3fcAj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#The ‘capcurve’ function that builds and shows the CAP curve is defined as follows :\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import integrate\n",
        "plt.switch_backend('agg')\n",
        "from scipy import integrate\n",
        "\n",
        "def capcurve(y_values, y_preds_proba):\n",
        "    num_pos_obs = np.sum(y_values)\n",
        "    num_count = len(y_values)\n",
        "    rate_pos_obs = float(num_pos_obs) / float(num_count)\n",
        "    ideal = pd.DataFrame({'x':[0,rate_pos_obs,1],'y':[0,1,1]})\n",
        "    xx = np.arange(num_count) / float(num_count - 1)\n",
        "    \n",
        "    y_cap = np.c_[y_values,y_preds_proba]\n",
        "    y_cap_df_s = pd.DataFrame(data=y_cap)\n",
        "    y_cap_df_s = y_cap_df_s.sort_values([1], ascending=False).reset_index(level = y_cap_df_s.index.names, drop=True)\n",
        "    \n",
        "    print(y_cap_df_s.head(20))\n",
        "    \n",
        "    yy = np.cumsum(y_cap_df_s[0]) / float(num_pos_obs)\n",
        "    yy = np.append([0], yy[0:num_count-1]) #add the first curve point (0,0) : for xx=0 we have yy=0\n",
        "    \n",
        "    percent = 0.5\n",
        "    row_index = int(np.trunc(num_count * percent))\n",
        "    \n",
        "    val_y1 = yy[row_index]\n",
        "    val_y2 = yy[row_index+1]\n",
        "    if val_y1 == val_y2:\n",
        "        val = val_y1*1.0\n",
        "    else:\n",
        "        val_x1 = xx[row_index]\n",
        "        val_x2 = xx[row_index+1]\n",
        "        val = val_y1 + ((val_x2 - percent)/(val_x2 - val_x1))*(val_y2 - val_y1)\n",
        "    \n",
        "    sigma_ideal = 1 * xx[num_pos_obs - 1 ] / 2 + (xx[num_count - 1] - xx[num_pos_obs]) * 1\n",
        "    sigma_model = integrate.simps(yy,xx)\n",
        "    sigma_random = integrate.simps(xx,xx)\n",
        "    \n",
        "    ar_value = (sigma_model - sigma_random) / (sigma_ideal - sigma_random)\n",
        "    \n",
        "    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
        "    ax.plot(ideal['x'],ideal['y'], color='grey', label='Perfect Model')\n",
        "    ax.plot(xx,yy, color='red', label='User Model')\n",
        "    ax.plot(xx,xx, color='blue', label='Random Model')\n",
        "    ax.plot([percent, percent], [0.0, val], color='green', linestyle='--', linewidth=1)\n",
        "    ax.plot([0, percent], [val, val], color='green', linestyle='--', linewidth=1, label=str(val*100)+'% of positive obs at '+str(percent*100)+'%')\n",
        "    \n",
        "    plt.xlim(0, 1.02)\n",
        "    plt.ylim(0, 1.25)\n",
        "    plt.title(\"CAP Curve - a_r value =\"+str(ar_value))\n",
        "    plt.xlabel('% of the data')\n",
        "    plt.ylabel('% of positive obs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "# And then call the function\n",
        "from matplotlib import cm\n",
        "y_pred_proba = rc.predict_proba(X=X_test)\n",
        "capcurve(y_values=y_test, y_preds_proba=y_pred_proba[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccXW4Xd35zv2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create Cut Points for Buckets**"
      ]
    },
    {
      "metadata": {
        "id": "XCdYkS5X52yp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'value': np.random.randint(1, 80, 20)}) \n",
        "df['group'] = pd.cut(df.value,\n",
        "                     bins=[0, 5, 31, 51, 80],\n",
        "                     labels=[\"very short\", \"short\", \"long\", \"very long\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WXZvwGQF6SXp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix Plot**"
      ]
    },
    {
      "metadata": {
        "id": "qMTEqwAO6WbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "1.\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IHCh1E2T6b-C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "2.\n",
        "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "j = 1\n",
        "\n",
        "for i in thresholds:\n",
        "    y_test_predictions_high_recall = y_pred_proba[:, 1] > i\n",
        "    \n",
        "    plt.subplot(3, 3, j)\n",
        "    j += 1\n",
        "    \n",
        "    # Compute confusion matrix\n",
        "    cnf_matrix = metrics.confusion_matrix(y_test, y_test_predictions_high_recall)\n",
        "    np.set_printoptions(precision=2)\n",
        "    \n",
        "    print('Accuracy of the testing data:', float(cnf_matrix[0,0] + cnf_matrix[1,1]) / float(cnf_matrix[0,1] + cnf_matrix[1,0] +\n",
        "         cnf_matrix[0,0] + cnf_matrix[1,1]))\n",
        "    print(\"Recall metric in the testing dataset @: \",i,'::', float(cnf_matrix[1,1]) / float((cnf_matrix[1,0] + cnf_matrix[1,1])))\n",
        "    print(\"Precision metric in the testing dataset: \", float(cnf_matrix[1,1]) / float((cnf_matrix[1,1] + cnf_matrix[0,1])))\n",
        "    print('___________________________________________________')\n",
        "    print('Recall metric on 0 values:', float(cnf_matrix[0,0] /(cnf_matrix[0,0] + cnf_matrix[0,1])))\n",
        "    print('Precision metric on 0 values:', float(cnf_matrix[0,0]/(cnf_matrix[0,0] + cnf_matrix[1,0])))\n",
        "    # Plot non-normalized confusion matrix\n",
        "    class_names = [0, 1]\n",
        "    plot_confusion_matrix(cnf_matrix, classes=class_names, title='Threshold >= %s'%i) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UFA4ff9-68YY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Learning Curves Plot**"
      ]
    },
    {
      "metadata": {
        "id": "vsYc8ld46-1Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "g = plot_learning_curve(logit,\"Logit learning curves\",X_train,y_train,cv=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sl1fiRyGDupo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Precision Recall Curve**"
      ]
    },
    {
      "metadata": {
        "id": "wAwrUKvNDxvI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "From sklearn.metrics import precision_recall_curve\n",
        "\n",
        "Precisions, recalls, thresholds = precision_recall_curve(y_train, y_predict)\n",
        "\n",
        "\n",
        "Def plot_precision_recall_vs_thresholds(precisions, recalls, thresholds):\n",
        "\tPlt.plot(thresholds, precisions[:-1], \"b--\", label = 'Precisions)\n",
        "\tPlt.plot(thresholds, recalls[:-1], \"g-\", label = 'Recalls')\n",
        "\tPlt.xlabe('Threshold')\n",
        "\tPlt.legend(loc = 'upper left')\n",
        "\tPlt.ylim([0,1])\n",
        "\t\n",
        "Plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "Plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ddO7T9fDhRK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Outlier Detector**"
      ]
    },
    {
      "metadata": {
        "id": "sonLYSihDjKL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def detect_outliers(df,n,features):\n",
        "    \"\"\"\n",
        "    Takes a dataframe df of features and returns a list of the indices\n",
        "    corresponding to the observationsb containing more than n outliers according\n",
        "    to the Tukey method.\n",
        "    \"\"\"\n",
        "    outlier_indices = []\n",
        "    \n",
        "    # iterate over features(columns)\n",
        "    for col in features:\n",
        "        # 1st quartile (25%)\n",
        "        Q1 = np.percentile(df[col], 25)\n",
        "        # 3rd quartile (75%)\n",
        "        Q3 = np.percentile(df[col],75)\n",
        "        # Interquartile range (IQR)\n",
        "        IQR = Q3 - Q1\n",
        "        \n",
        "        # outlier step\n",
        "        outlier_step = 1.5 * IQR\n",
        "        \n",
        "        # Determine a list of indices of outliers for feature col\n",
        "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
        "        \n",
        "        # append the found outlier indices for col to the list of outlier indices \n",
        "        outlier_indices.extend(outlier_list_col)\n",
        "        \n",
        "    # select observations containing more than 2 outliers\n",
        "    outlier_indices = Counter(outlier_indices)        \n",
        "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
        "    \n",
        "    return multiple_outliers  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rZVjyF2kEFNl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Recursive Feature Elimination**"
      ]
    },
    {
      "metadata": {
        "id": "FQ997xheEHCW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Pick the most important feature names\n",
        "ranked_features = sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), X_train.columns[1:]))\n",
        "filtered_features = filter(lambda t: t[0]==1, ranked_features)\n",
        "feature_columns = [features[1] for features in ranked_features if features[0] < 11] # This value determines most important values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BsGVcFwnEUgY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Permutation Importance ELI5**"
      ]
    },
    {
      "metadata": {
        "id": "LQhjNgvTEYzv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "perm = PermutationImportance(logit, random_state=1).fit(X_train, y_train)\n",
        "eli5.show_weights(perm, feature_names = X_train.columns.tolist(), top = None)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}